{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4320e5",
   "metadata": {},
   "source": [
    "# Tutorial: Using SLDA to simulate Continual Learning Scenarios\n",
    "## With a frozen pretrained feature extractor\n",
    "Streaming Linear Discriminant Analysis (SLDA), is a type of generative model that learns a linear classifier over precomputed features from a frozen feature extractor.\n",
    "\n",
    "SLDA learns a per-class Gaussian distribution with covariance matrix that is shared across all classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb8a37",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41b15c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sunilach/openfl/fl_cl_ebm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf886d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunilach/flclenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-11-28 16:35:49.741741: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-11-28 16:35:50.300638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 17830 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Config/Options\n",
    "from config import Decoders\n",
    "from config import IMG_AUGMENT_LAYERS\n",
    "\n",
    "# Model/Loss definitions\n",
    "from models.slda import SLDA\n",
    "from models import losses\n",
    "from models.utils import extract_features\n",
    "\n",
    "# Dataset handling (synthesize/build/query)\n",
    "from lib.dataset.repository import DatasetRepository\n",
    "from lib.dataset.utils import as_tuple, decode_example, get_label_distribution\n",
    "from lib.dataset.synthesizer import synthesize_by_sharding_over_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade995f",
   "metadata": {},
   "source": [
    "### Experiment Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0304ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10' #' caltech_birds2011'   # If loading a public TensorFlow dataset\n",
    "# DATASET = '/tmp/repository/vege'  # If loading a local TFRecord dataset\n",
    "\n",
    "IMG_SIZE = (32, 32)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 16384\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed4d6b",
   "metadata": {},
   "source": [
    "### Load the *entire* Dataset\n",
    "We deal with `tf.data.Dataset` APIs for all our simulations.\n",
    "\n",
    "The additional argument to note here, is the `decoders`. We supply our custom `Decoders.SIMPLE_DECODER` that partially decodes the data for two main reasons:\n",
    "1. It only parses `image` and `label` keys from the dataset (we're only dealing with classification problems here).\n",
    "2. It 'skips' decoding the images to tensors (hence you see it as `tf.string` type). This is for performance reasons. As you'll see, we decode it when we build our data pipeline for training/testing on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae70763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About:  tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    full_name='cifar10/3.0.2',\n",
      "    description=\"\"\"\n",
      "    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "    \"\"\",\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    data_path='/home/sunilach/tensorflow_datasets/cifar10/3.0.2',\n",
      "    download_size=162.17 MiB,\n",
      "    dataset_size=132.40 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      ")\n",
      "Element Spec:  {'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}\n",
      "Training samples:  50000\n",
      "Testing samples:  10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load the dataset: Public or Local\"\"\"\n",
    "if tf.io.gfile.isdir(DATASET):\n",
    "    repo = DatasetRepository(data_dir=DATASET)\n",
    "    builder = repo.get_builder()  # Builds all versions by default\n",
    "    ds_info = builder.info\n",
    "    (raw_train_ds, raw_test_ds) = builder.as_dataset(split=['train', 'test'],\n",
    "                                                     decoders=Decoders.SIMPLE_DECODER)\n",
    "else:\n",
    "    # Load TFDS dataset by name (publicly-hosted on TF)\n",
    "    (raw_train_ds, raw_test_ds), ds_info = tfds.load(DATASET,\n",
    "                                                     split=['train', 'test'],\n",
    "                                                     with_info=True,\n",
    "                                                     decoders=Decoders.SIMPLE_DECODER)\n",
    "print('About: ', ds_info)\n",
    "print('Element Spec: ', raw_train_ds.element_spec)\n",
    "print('Training samples: ', len(raw_train_ds))\n",
    "print('Testing samples: ', len(raw_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1477d9a",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Let's choose a pretrained backbone to extract features. Since in this experiment we keep the backbone frozen and finetune only a few additional layers, it is much faster to iterate if we compute all features of all images at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce27ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " augment_layers (Sequential)  (None, 32, 32, 3)        0         \n",
      "                                                                 \n",
      " efficientnetv2-b0 (Function  (None, 1280)             5919312   \n",
      " al)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,919,312\n",
      "Trainable params: 0\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Choose Model backbone to extract features\"\"\"\n",
    "backbone = tf.keras.applications.EfficientNetV2B0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "backbone.trainable = False\n",
    "\n",
    "\"\"\"Add augmentation/input layers\"\"\"\n",
    "feature_extractor = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(backbone.input_shape[1:]),\n",
    "    IMG_AUGMENT_LAYERS,\n",
    "    backbone,\n",
    "], name='feature_extractor')\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train set features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-28 16:35:54.612019: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 42s 24ms/step\n",
      "Extracting test set features\n",
      "313/313 [==============================] - 8s 25ms/step\n",
      "Features Dataset spec:  {'image': TensorSpec(shape=(1280,), dtype=tf.float32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Extract train/test feature embeddings\"\"\"\n",
    "print(f'Extracting train set features')\n",
    "train_features = extract_features(dataset=(raw_train_ds\n",
    "                                        .map(decode_example(IMG_SIZE))\n",
    "                                        .map(as_tuple(x='image', y='label'))\n",
    "                                        .batch(BATCH_SIZE)\n",
    "                                        .prefetch(tf.data.AUTOTUNE)), model=feature_extractor)\n",
    "print(f'Extracting test set features')\n",
    "test_features = extract_features(dataset=(raw_test_ds\n",
    "                                        .map(decode_example(IMG_SIZE))\n",
    "                                        .map(as_tuple(x='image', y='label'))\n",
    "                                        .batch(BATCH_SIZE)\n",
    "                                        .prefetch(tf.data.AUTOTUNE)), model=feature_extractor)\n",
    "print('Features Dataset spec: ', train_features.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c3174",
   "metadata": {},
   "source": [
    "### Creating a Continual Learning Dataset\n",
    "Now that we have the extracted features, we would like to partition this entire training set into `n` parts, to train our model sequentially, without access to older data.\n",
    "\n",
    "Each partition holds data from only a selected few classes. In literature, this is known as the 'Class Incremental Learning' setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74243f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partitions: 5\n",
      "Partition 0: {4: 5000, 6: 5000}\n",
      "Partition 1: {2: 5000, 7: 5000}\n",
      "Partition 2: {3: 5000, 5: 5000}\n",
      "Partition 3: {0: 5000, 9: 5000}\n",
      "Partition 4: {1: 5000, 8: 5000}\n"
     ]
    }
   ],
   "source": [
    "N_PARTITIONS = 5\n",
    "\n",
    "# This returns a dictionary of partitioned datasets, keyed by partition_id, an integer\n",
    "partitioned_dataset = synthesize_by_sharding_over_labels(train_features, \n",
    "                                                         num_partitions=N_PARTITIONS, \n",
    "                                                         shuffle_labels=True)\n",
    "# Check the label counts of each partition\n",
    "print('Partitions:', len(partitioned_dataset))\n",
    "for partition_id in partitioned_dataset:\n",
    "    dist = get_label_distribution(partitioned_dataset[partition_id])\n",
    "    print(f'Partition {partition_id}: {dist}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8526a2",
   "metadata": {},
   "source": [
    "### Define an SLDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cfd85893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[10000.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0., 10000.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0.,     0., 10000., ...,     0.,     0.,     0.],\n",
       "        ...,\n",
       "        [    0.,     0.,     0., ..., 10000.,     0.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0., 10000.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0.,     0., 10000.]],\n",
       "       dtype=float32),\n",
       " 0.0,\n",
       " False]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SLDA takes a feature vector, linearly maps it to the output class\n",
    "model = SLDA(n_components=feature_extractor.output_shape[-1],\n",
    "             num_classes=ds_info.features['label'].num_classes)\n",
    "\n",
    "# Compile. No loss/optimizer since it is a gradient-free algorithm\n",
    "model.compile(metrics=['accuracy'])\n",
    "\n",
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9ab3f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cloudpickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3fd71650",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "702ded91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(10, activation=None),\n",
    "# ], name='simplecnn')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aebf6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = tf.keras.Sequential()\n",
    "# model1.add(tf.keras.layers.Dense(8, input_shape=(8,)))\n",
    "# model1.add(tf.keras.layers.Dense(1))\n",
    "# # model1.compile(optimizer='sgd', loss='mse')\n",
    "# # model1 = copy.deepcopy(model1)\n",
    "# # model1.summary()\n",
    "# model1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4af7adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filename = 'cp_keras_model1'\n",
    "# with open(filename, 'wb') as f:\n",
    "#     cloudpickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2730c4d9",
   "metadata": {},
   "source": [
    "### Train SLDA Model sequentially over each Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "daf22d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10001/10001 [==============================] - 26s 3ms/step\n",
      "10000/10000 [==============================] - 34s 3ms/step - accuracy: 0.9306\n",
      "Train Acc:  0.9305999875068665\n"
     ]
    }
   ],
   "source": [
    "# Build test dataset pipeline\n",
    "test_ds = (test_features\n",
    "            .cache()\n",
    "            .map(as_tuple(x='image', y='label'))\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Incrementally train on each partition\n",
    "# for partition_id in partitioned_dataset:\n",
    "\n",
    "#     print(f'Training [{partition_id+1}/{len(partitioned_dataset)}]')\n",
    "\n",
    "\n",
    "    # Build Train Dataset pipeline\n",
    "train_ds = (partitioned_dataset[partition_id]\n",
    "            .cache()\n",
    "            .shuffle(SHUFFLE_BUFFER)\n",
    "            .map(as_tuple(x='image', y='label'))\n",
    "            .batch(1)  # SLDA learns 1-sample at a time. Inference can be done on batch.\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# SLDA performs well even on a single pass over the dataset\n",
    "model.fit(train_ds, epochs=1)\n",
    "print(\"Train Acc: \", model.evaluate(train_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b435e311",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e964f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe5bdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d1b1ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392d84d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76443161",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Try testing various partition sizes for SLDA. You'll observe the drop in accuracy isn't significant despite multiple tasks.\n",
    "This is due to the generative nature of LDA.\n",
    "\n",
    "By learning per-class Gaussians, class-incremental learning problem becomes task-incremental, making it agnostic of the order of classes during training."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
