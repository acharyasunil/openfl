{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b4320e5",
   "metadata": {},
   "source": [
    "# FedSLDA: With known class IDs across clients (no reconciliation)\n",
    "Streaming Linear Discriminant Analysis (SLDA), is a type of generative model that learns a linear classifier over precomputed features from a frozen feature extractor.\n",
    "\n",
    "SLDA learns a per-class Gaussian distribution with covariance matrix that is shared across all classes. \n",
    "\n",
    "Objective in this notebook, is to train multiple SLDA models over synthetically-generated client datasets, average them, and obtain an aggregated model. Rinse, repeat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2acb8a37",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c16be406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/sunilach/openfl/forked-intel-openfl/openfl/cl/')\n",
    "# sys.path.append('/home/sunilach/openfl/fl_cl_ebm/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cf886d95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 09:36:55.278918: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/sunilach/flclenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-20 09:36:56.997091: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 09:36:57.563031: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-12-20 09:36:57.563083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14097 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Config/Options\n",
    "from config import Decoders\n",
    "from config import IMG_AUGMENT_LAYERS\n",
    "\n",
    "# Model/Loss definitions\n",
    "from models.slda import SLDA\n",
    "from models import losses\n",
    "from models.utils import extract_features\n",
    "\n",
    "# Dataset handling (synthesize/build/query)\n",
    "from lib.dataset.repository import DatasetRepository\n",
    "from lib.dataset.utils import as_tuple, decode_example, get_label_distribution\n",
    "from lib.dataset.synthesizer import synthesize_by_sharding_over_labels\n",
    "from lib.dataset.synthesizer import synthesize_by_dirichlet_over_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ade995f",
   "metadata": {},
   "source": [
    "### Experiment Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0304ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'   # If loading a public TensorFlow dataset\n",
    "# DATASET = '/tmp/repository/vege'  # If loading a local TFRecord dataset\n",
    "\n",
    "IMG_SIZE = (32, 32)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 16384\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed4d6b",
   "metadata": {},
   "source": [
    "### Load the *entire* Dataset\n",
    "We deal with `tf.data.Dataset` APIs for all our simulations.\n",
    "\n",
    "The additional argument to note here, is the `decoders`. We supply our custom `Decoders.SIMPLE_DECODER` that partially decodes the data for two main reasons:\n",
    "1. It only parses `image` and `label` keys from the dataset (we're only dealing with classification problems here).\n",
    "2. It 'skips' decoding the images to tensors (hence you see it as `tf.string` type). This is for performance reasons. As you'll see, we decode it when we build our data pipeline for training/testing on-the-fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ae70763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About:  tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    full_name='cifar10/3.0.2',\n",
      "    description=\"\"\"\n",
      "    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "    \"\"\",\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    data_path='/home/sunilach/tensorflow_datasets/cifar10/3.0.2',\n",
      "    download_size=162.17 MiB,\n",
      "    dataset_size=132.40 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      ")\n",
      "Element Spec:  {'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}\n",
      "Training samples:  50000\n",
      "Testing samples:  10000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Load the dataset: Public or Local\"\"\"\n",
    "if tf.io.gfile.isdir(DATASET):\n",
    "    repo = DatasetRepository(data_dir=DATASET)\n",
    "    builder = repo.get_builder()  # Builds all versions by default\n",
    "    ds_info = builder.info\n",
    "    (raw_train_ds, raw_test_ds) = builder.as_dataset(split=['train', 'test'],\n",
    "                                                     decoders=Decoders.SIMPLE_DECODER)\n",
    "else:\n",
    "    # Load TFDS dataset by name (publicly-hosted on TF)\n",
    "    (raw_train_ds, raw_test_ds), ds_info = tfds.load(DATASET,\n",
    "                                                     split=['train', 'test'],\n",
    "                                                     with_info=True,\n",
    "                                                     decoders=Decoders.SIMPLE_DECODER)\n",
    "print('About: ', ds_info)\n",
    "print('Element Spec: ', raw_train_ds.element_spec)\n",
    "print('Training samples: ', len(raw_train_ds))\n",
    "print('Testing samples: ', len(raw_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1477d9a",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "Let's choose a pretrained backbone to extract features. Since in this experiment we keep the backbone frozen and finetune only a few additional layers, it is much faster to iterate if we compute all features of all images at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ce27ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " augment_layers (Sequential)  (None, 32, 32, 3)        0         \n",
      "                                                                 \n",
      " efficientnetv2-b0 (Function  (None, 1280)             5919312   \n",
      " al)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,919,312\n",
      "Trainable params: 0\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Choose Model backbone to extract features\"\"\"\n",
    "backbone = tf.keras.applications.EfficientNetV2B0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(*IMG_SIZE, 3),\n",
    "    pooling='avg'\n",
    ")\n",
    "backbone.trainable = False\n",
    "\n",
    "\"\"\"Add augmentation/input layers\"\"\"\n",
    "feature_extractor = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(backbone.input_shape[1:]),\n",
    "    IMG_AUGMENT_LAYERS,\n",
    "    backbone,\n",
    "], name='feature_extractor')\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8f786c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting train set features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 09:37:02.373892: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 19s 10ms/step\n",
      "Extracting test set features\n",
      "313/313 [==============================] - 3s 10ms/step\n",
      "Features Dataset spec:  {'image': TensorSpec(shape=(1280,), dtype=tf.float32, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Extract train/test feature embeddings\"\"\"\n",
    "print(f'Extracting train set features')\n",
    "train_features = extract_features(dataset=(raw_train_ds\n",
    "                                        .map(decode_example(IMG_SIZE))\n",
    "                                        .map(as_tuple(x='image', y='label'))\n",
    "                                        .batch(BATCH_SIZE)\n",
    "                                        .prefetch(tf.data.AUTOTUNE)), model=feature_extractor)\n",
    "print(f'Extracting test set features')\n",
    "test_features = extract_features(dataset=(raw_test_ds\n",
    "                                        .map(decode_example(IMG_SIZE))\n",
    "                                        .map(as_tuple(x='image', y='label'))\n",
    "                                        .batch(BATCH_SIZE)\n",
    "                                        .prefetch(tf.data.AUTOTUNE)), model=feature_extractor)\n",
    "print('Features Dataset spec: ', train_features.element_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0c3174",
   "metadata": {},
   "source": [
    "### Creating a Federated Dataset\n",
    "Now that we have the extracted features, we would like to partition this entire training set into `n` parts, `n` should be reminiscent of the number of participants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74243f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clients: 7\n",
      "Client 0: {4: 5000}\n",
      "Client 1: {6: 5000}\n",
      "Client 2: {2: 5000}\n",
      "Client 3: {7: 5000}\n",
      "Client 4: {3: 5000}\n",
      "Client 5: {5: 5000}\n",
      "Client 6: {9: 5000}\n"
     ]
    }
   ],
   "source": [
    "N_CLIENTS = 7\n",
    "\n",
    "# This returns a dictionary of partitioned datasets for each client, keyed by client_id, an integer\n",
    "client_datasets = synthesize_by_sharding_over_labels(train_features, \n",
    "                                                         num_partitions=N_CLIENTS, \n",
    "                                                         shuffle_labels=True)\n",
    "# client_datasets = synthesize_by_dirichlet_over_labels(train_features, \n",
    "#                                                       num_partitions=N_CLIENTS, \n",
    "#                                                       concentration_factor=0.1)\n",
    "\n",
    "# Check the label counts of each partition\n",
    "print('Clients:', len(client_datasets))\n",
    "for client_id in client_datasets:\n",
    "    dist = get_label_distribution(client_datasets[client_id])\n",
    "    print(f'Client {client_id}: {dist}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8526a2",
   "metadata": {},
   "source": [
    "### Define `N_CLIENTS` SLDA Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cfd85893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "{0: <models.slda.SLDA object at 0x7f7d800d9460>, 1: <models.slda.SLDA object at 0x7f7d607b2250>, 2: <models.slda.SLDA object at 0x7f7d60782430>, 3: <models.slda.SLDA object at 0x7f7d800f16a0>, 4: <models.slda.SLDA object at 0x7f7d800e74f0>, 5: <models.slda.SLDA object at 0x7f7d606fadc0>, 6: <models.slda.SLDA object at 0x7f7d60751460>}\n"
     ]
    }
   ],
   "source": [
    "# [Zero Mean/Std] Initialize N_CLIENTS candidate models\n",
    "client_models = {\n",
    "    i: SLDA(n_components=feature_extractor.output_shape[-1],\n",
    "             num_classes=ds_info.features['label'].num_classes)\n",
    "    for i in range(len(client_datasets))\n",
    "}\n",
    "print(client_models)\n",
    "\n",
    "# Compile all client models. No loss/optimizer since it is a gradient-free algorithm\n",
    "_ = [\n",
    "    client_models[i].compile(metrics=['accuracy']) \n",
    "    for i in range(len(client_datasets))\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18490de9",
   "metadata": {},
   "source": [
    "### Federated Helper Functions\n",
    "For aggregation, broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65127ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate(models: dict):\n",
    "    \"\"\"Aggregates the dictionary of SLDA models\"\"\"\n",
    "    # Initialize fresh model\n",
    "    aggregated_model = SLDA(n_components=feature_extractor.output_shape[-1],\n",
    "                        num_classes=ds_info.features['label'].num_classes)\n",
    "    \n",
    "    # Total datapoints across clients\n",
    "    total_datapoints = tf.math.add_n([\n",
    "        models[i].counts\n",
    "        for i in models\n",
    "    ])\n",
    "    print('Total datapoints across all clients: ', total_datapoints)\n",
    "    \n",
    "    # Weightage per client\n",
    "    weightage_per_client = {\n",
    "        i: tf.expand_dims(models[i].counts / total_datapoints, -1)\n",
    "        for i in models\n",
    "    }\n",
    "    \n",
    "    print('Weightage per client: ')\n",
    "    print(weightage_per_client)\n",
    "\n",
    "    # Mean of mean\n",
    "    aggregated_model.means.assign(tf.math.add_n([\n",
    "        weightage_per_client[i] * models[i].means\n",
    "        for i in models\n",
    "    ]))\n",
    "    \n",
    "    # Add up covariances (TODO: Not sure if this mathematically holds)\n",
    "    aggregated_model.sigma.assign(tf.math.add_n([\n",
    "        models[i].sigma\n",
    "        for i in models\n",
    "    ]))\n",
    "    \n",
    "    return aggregated_model\n",
    "\n",
    "def broadcast(aggregated_model, client_models):\n",
    "    \"\"\"Copies Mean/Sigma from `aggregated_model`\n",
    "    and resets Counts of SLDA models\"\"\"\n",
    "    for client_id in client_models:\n",
    "        client_models[client_id].means.assign(aggregated_model.means)\n",
    "        client_models[client_id].sigma.assign(aggregated_model.sigma)\n",
    "        client_models[client_id].counts.assign(tf.zeros_like(client_models[client_id].counts))\n",
    "#         client_models[client_id]._steps.assign(0)  # Necessary?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db99c8",
   "metadata": {},
   "source": [
    "### Federated Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2634e37f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 0\n",
      "Training Client 0\n",
      "5001/5001 [==============================] - 5s 914us/step - val_accuracy: 0.1000\n",
      "Training Client 1\n",
      "5001/5001 [==============================] - 5s 906us/step - val_accuracy: 0.1000\n",
      "Training Client 2\n",
      "5001/5001 [==============================] - 5s 907us/step - val_accuracy: 0.1000\n",
      "Training Client 3\n",
      "5001/5001 [==============================] - 5s 900us/step - val_accuracy: 0.1000\n",
      "Training Client 4\n",
      "5001/5001 [==============================] - 5s 914us/step - val_accuracy: 0.1000\n",
      "Training Client 5\n",
      "5001/5001 [==============================] - 5s 900us/step - val_accuracy: 0.1000\n",
      "Training Client 6\n",
      "5001/5001 [==============================] - 5s 901us/step - val_accuracy: 0.1000\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "Total datapoints across all clients:  tf.Tensor([   0.    0. 5000. 5000. 5000. 5000. 5000. 5000.    0. 5000.], shape=(10,), dtype=float32)\n",
      "Weightage per client: \n",
      "{0: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 1: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 2: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 3: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 4: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 5: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 6: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 1.]], dtype=float32)>}\n",
      "313/313 [==============================] - 0s 978us/step - accuracy: 0.3566\n",
      "Aggregated model accuracy: 0.35659998655319214\n",
      "Round 1\n",
      "Training Client 0\n",
      "5001/5001 [==============================] - 4s 893us/step - val_accuracy: 0.4515\n",
      "Training Client 1\n",
      "5001/5001 [==============================] - 4s 889us/step - val_accuracy: 0.4527\n",
      "Training Client 2\n",
      "5001/5001 [==============================] - 4s 887us/step - val_accuracy: 0.4491\n",
      "Training Client 3\n",
      "5001/5001 [==============================] - 4s 883us/step - val_accuracy: 0.4515\n",
      "Training Client 4\n",
      "5001/5001 [==============================] - 4s 898us/step - val_accuracy: 0.4495\n",
      "Training Client 5\n",
      "5001/5001 [==============================] - 4s 885us/step - val_accuracy: 0.4486\n",
      "Training Client 6\n",
      "5001/5001 [==============================] - 5s 901us/step - val_accuracy: 0.4518\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "Total datapoints across all clients:  tf.Tensor([   0.    0. 5000. 5000. 5000. 5000. 5000. 5000.    0. 5000.], shape=(10,), dtype=float32)\n",
      "Weightage per client: \n",
      "{0: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 1: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 2: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 3: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 4: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 5: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 6: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 1.]], dtype=float32)>}\n",
      "313/313 [==============================] - 0s 970us/step - accuracy: 0.3566\n",
      "Aggregated model accuracy: 0.35659998655319214\n",
      "Round 2\n",
      "Training Client 0\n",
      "5001/5001 [==============================] - 4s 889us/step - val_accuracy: 0.4510\n",
      "Training Client 1\n",
      "5001/5001 [==============================] - 4s 884us/step - val_accuracy: 0.4524\n",
      "Training Client 2\n",
      "5001/5001 [==============================] - 4s 890us/step - val_accuracy: 0.4515\n",
      "Training Client 3\n",
      "5001/5001 [==============================] - 4s 888us/step - val_accuracy: 0.4514\n",
      "Training Client 4\n",
      "5001/5001 [==============================] - 4s 894us/step - val_accuracy: 0.4516\n",
      "Training Client 5\n",
      "5001/5001 [==============================] - 4s 878us/step - val_accuracy: 0.4516\n",
      "Training Client 6\n",
      "5001/5001 [==============================] - 4s 892us/step - val_accuracy: 0.4519\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "Total datapoints across all clients:  tf.Tensor([   0.    0. 5000. 5000. 5000. 5000. 5000. 5000.    0. 5000.], shape=(10,), dtype=float32)\n",
      "Weightage per client: \n",
      "{0: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 1: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 2: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 3: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 4: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 5: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 6: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 1.]], dtype=float32)>}\n",
      "313/313 [==============================] - 0s 983us/step - accuracy: 0.3566\n",
      "Aggregated model accuracy: 0.35659998655319214\n",
      "Round 3\n",
      "Training Client 0\n",
      "5001/5001 [==============================] - 4s 891us/step - val_accuracy: 0.4513\n",
      "Training Client 1\n",
      "5001/5001 [==============================] - 4s 883us/step - val_accuracy: 0.4512\n",
      "Training Client 2\n",
      "5001/5001 [==============================] - 4s 883us/step - val_accuracy: 0.4512\n",
      "Training Client 3\n",
      "5001/5001 [==============================] - 4s 882us/step - val_accuracy: 0.4515\n",
      "Training Client 4\n",
      "5001/5001 [==============================] - 4s 894us/step - val_accuracy: 0.4513\n",
      "Training Client 5\n",
      "5001/5001 [==============================] - 4s 881us/step - val_accuracy: 0.4519\n",
      "Training Client 6\n",
      "5001/5001 [==============================] - 4s 894us/step - val_accuracy: 0.4514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "Total datapoints across all clients:  tf.Tensor([   0.    0. 5000. 5000. 5000. 5000. 5000. 5000.    0. 5000.], shape=(10,), dtype=float32)\n",
      "Weightage per client: \n",
      "{0: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 1: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 2: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 3: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 4: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 5: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 6: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 1.]], dtype=float32)>}\n",
      "313/313 [==============================] - 0s 979us/step - accuracy: 0.3566\n",
      "Aggregated model accuracy: 0.35659998655319214\n",
      "Round 4\n",
      "Training Client 0\n",
      "5001/5001 [==============================] - 4s 890us/step - val_accuracy: 0.4520\n",
      "Training Client 1\n",
      "5001/5001 [==============================] - 4s 888us/step - val_accuracy: 0.4519\n",
      "Training Client 2\n",
      "5001/5001 [==============================] - 4s 883us/step - val_accuracy: 0.4519\n",
      "Training Client 3\n",
      "5001/5001 [==============================] - 4s 884us/step - val_accuracy: 0.4519\n",
      "Training Client 4\n",
      "5001/5001 [==============================] - 4s 887us/step - val_accuracy: 0.4523\n",
      "Training Client 5\n",
      "5001/5001 [==============================] - 4s 879us/step - val_accuracy: 0.4519\n",
      "Training Client 6\n",
      "5001/5001 [==============================] - 5s 912us/step - val_accuracy: 0.4522\n",
      "BEFORE BUILD SLDA\n",
      "AFTER BUILD SLDA\n",
      "Total datapoints across all clients:  tf.Tensor([   0.    0. 5000. 5000. 5000. 5000. 5000. 5000.    0. 5000.], shape=(10,), dtype=float32)\n",
      "Weightage per client: \n",
      "{0: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 1: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 2: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 3: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 4: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 5: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 1.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 0.]], dtype=float32)>, 6: <tf.Tensor: shape=(10, 1), dtype=float32, numpy=\n",
      "array([[nan],\n",
      "       [nan],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [ 0.],\n",
      "       [nan],\n",
      "       [ 1.]], dtype=float32)>}\n",
      "313/313 [==============================] - 0s 979us/step - accuracy: 0.3566\n",
      "Aggregated model accuracy: 0.35659998655319214\n"
     ]
    }
   ],
   "source": [
    "# Build test dataset pipeline (common for all)\n",
    "test_ds = (test_features\n",
    "            .cache()\n",
    "            .map(as_tuple(x='image', y='label'))\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "# Train multiple rounds\n",
    "N_ROUNDS = 5\n",
    "\n",
    "for r in range(N_ROUNDS):\n",
    "    print(f'Round {r}')\n",
    "    \n",
    "    # Train each client on its respective dataset\n",
    "    for client_id in client_datasets:\n",
    "        print(f'Training Client {client_id}')\n",
    "        client_models[client_id].fit((client_datasets[client_id]\n",
    "                                    .cache()\n",
    "                                    .shuffle(SHUFFLE_BUFFER)\n",
    "                                    .map(as_tuple(x='image', y='label'))\n",
    "                                    .batch(1)  # SLDA learns 1-sample at a time. Inference can be done on batch.\n",
    "                                    .prefetch(tf.data.AUTOTUNE)), epochs=1, validation_data=test_ds)\n",
    "    \n",
    "    # Aggregate model at end of round\n",
    "    aggregated_model = aggregate(client_models)\n",
    "    aggregated_model.compile(metrics=['accuracy'])\n",
    "    acc = aggregated_model.evaluate(test_ds)\n",
    "    print('Aggregated model accuracy:', acc)\n",
    "    \n",
    "    # Broadcast models to all clients\n",
    "    broadcast(aggregated_model, client_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76443161",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "Regardless of the heterogeneity and number of clients, convergence is one-shot. Why? This needs to be confirmed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17be84f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "37dae9d8f6459649a063d6171dd4638ba9563c24ba3bafb3df8fc69f39dab02b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
