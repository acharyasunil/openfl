{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26fdd9ed",
   "metadata": {},
   "source": [
    "# Federated SLDA TF.Data and Custom Dataset Tutorial\n",
    "Using `tf.data` API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1329f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install TF if not already. We recommend TF2.7 or greater.\n",
    "# !pip install tensorflow==2.8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d30942",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "441252f7-ed37-4391-a466-7c9ba7054cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TF_FORCE_GPU_ALLOW_GROWTH=true\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 10:26:30.868694: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "/home/sunilach/flclenv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-20 10:26:32.601269: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-20 10:26:33.152821: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:42] Overriding orig_value setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
      "2022-12-20 10:26:33.152873: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9763 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:18:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "%env TF_FORCE_GPU_ALLOW_GROWTH=true\n",
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Config/Options\n",
    "from openfl.cl.config import Decoders\n",
    "from openfl.cl.config import IMG_AUGMENT_LAYERS\n",
    "\n",
    "# Model/Loss definitions\n",
    "from openfl.cl.models.slda import SLDA\n",
    "from openfl.cl.models import losses\n",
    "from openfl.cl.models.utils import extract_features\n",
    "\n",
    "# Dataset handling (synthesize/build/query)\n",
    "from openfl.cl.lib.dataset.repository import DatasetRepository\n",
    "from openfl.cl.lib.dataset.utils import as_tuple, decode_example, get_label_distribution\n",
    "from openfl.cl.lib.dataset.synthesizer import synthesize_by_sharding_over_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0833dfc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow 2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print('TensorFlow', tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a69ac1b-115c-4e4b-99c5-e7471d17ffb7",
   "metadata": {},
   "source": [
    "Experiment Options\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "967f73ed-d4f7-4bd1-a051-6ea9dc9a708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET = 'cifar10'   # If loading a public TensorFlow dataset\n",
    "# DATASET = '/tmp/repository/vege'  # If loading a local TFRecord dataset\n",
    "\n",
    "IMG_SIZE = (32, 32)\n",
    "BATCH_SIZE = 32\n",
    "SHUFFLE_BUFFER = 16384"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246f9c98",
   "metadata": {},
   "source": [
    "## Connect to the Federation\n",
    "\n",
    "Start `Director` and `Envoy` before proceeding with this cell. \n",
    "\n",
    "This cell connects this notebook to the Federation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d657e463",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.federation import Federation\n",
    "\n",
    "# please use the same identificator that was used in signed certificate\n",
    "client_id = 'api'\n",
    "cert_dir = 'cert'\n",
    "director_node_fqdn = 'localhost'\n",
    "director_port = 50051\n",
    "\n",
    "# Create a Federation\n",
    "federation = Federation(\n",
    "    client_id=client_id,\n",
    "    director_node_fqdn=director_node_fqdn,\n",
    "    director_port=director_port, \n",
    "    tls=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efe22a8",
   "metadata": {},
   "source": [
    "## Query Datasets from Shard Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "47dcfab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q1': {'shard_info': node_info {\n",
       "    name: \"Q1\"\n",
       "  }\n",
       "  shard_description: \"CIFAR10 dataset, shard number 1/3.\\nSamples [Train/Valid]: [15000/10000]\"\n",
       "  sample_shape: \"32\"\n",
       "  sample_shape: \"32\"\n",
       "  sample_shape: \"3\"\n",
       "  target_shape: \"1\",\n",
       "  'is_online': True,\n",
       "  'is_experiment_running': False,\n",
       "  'last_updated': '2022-12-20 10:26:21',\n",
       "  'current_time': '2022-12-20 10:26:33',\n",
       "  'valid_duration': seconds: 120,\n",
       "  'experiment_name': 'ExperimentName Mock'},\n",
       " 'Q2': {'shard_info': node_info {\n",
       "    name: \"Q2\"\n",
       "  }\n",
       "  shard_description: \"CIFAR10 dataset, shard number 2/3.\\nSamples [Train/Valid]: [15000/10000]\"\n",
       "  sample_shape: \"32\"\n",
       "  sample_shape: \"32\"\n",
       "  sample_shape: \"3\"\n",
       "  target_shape: \"1\",\n",
       "  'is_online': True,\n",
       "  'is_experiment_running': False,\n",
       "  'last_updated': '2022-12-20 10:26:26',\n",
       "  'current_time': '2022-12-20 10:26:33',\n",
       "  'valid_duration': seconds: 120,\n",
       "  'experiment_name': 'ExperimentName Mock'},\n",
       " 'Q3': {'shard_info': node_info {\n",
       "    name: \"Q3\"\n",
       "  }\n",
       "  shard_description: \"CIFAR10 dataset, shard number 3/3.\\nSamples [Train/Valid]: [15000/10000]\"\n",
       "  sample_shape: \"32\"\n",
       "  sample_shape: \"32\"\n",
       "  sample_shape: \"3\"\n",
       "  target_shape: \"1\",\n",
       "  'is_online': True,\n",
       "  'is_experiment_running': False,\n",
       "  'last_updated': '2022-12-20 10:26:31',\n",
       "  'current_time': '2022-12-20 10:26:33',\n",
       "  'valid_duration': seconds: 120,\n",
       "  'experiment_name': 'ExperimentName Mock'}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shard_registry = federation.get_shard_registry()\n",
    "shard_registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2a6c237",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sample shape: (32, 32, 3), target shape: (1,)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# First, request a dummy_shard_desc that holds information about the federated dataset \n",
    "dummy_shard_desc = federation.get_dummy_shard_descriptor(size=10)\n",
    "dummy_shard_dataset = dummy_shard_desc.get_dataset('train')\n",
    "sample, target = dummy_shard_dataset[0]\n",
    "f\"Sample shape: {sample.shape}, target shape: {target.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0dbdbd",
   "metadata": {},
   "source": [
    "## Describing FL experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc88700a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import TaskInterface\n",
    "from openfl.interface.interactive_api.experiment import ModelInterface\n",
    "from openfl.interface.interactive_api.experiment import FLExperiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0979470",
   "metadata": {},
   "source": [
    "### Register dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26642f79-2c55-41ff-a0e8-fbbe5084f378",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "About:  tfds.core.DatasetInfo(\n",
      "    name='cifar10',\n",
      "    full_name='cifar10/3.0.2',\n",
      "    description=\"\"\"\n",
      "    The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
      "    \"\"\",\n",
      "    homepage='https://www.cs.toronto.edu/~kriz/cifar.html',\n",
      "    data_path='/home/sunilach/tensorflow_datasets/cifar10/3.0.2',\n",
      "    download_size=162.17 MiB,\n",
      "    dataset_size=132.40 MiB,\n",
      "    features=FeaturesDict({\n",
      "        'id': Text(shape=(), dtype=tf.string),\n",
      "        'image': Image(shape=(32, 32, 3), dtype=tf.uint8),\n",
      "        'label': ClassLabel(shape=(), dtype=tf.int64, num_classes=10),\n",
      "    }),\n",
      "    supervised_keys=('image', 'label'),\n",
      "    disable_shuffling=False,\n",
      "    splits={\n",
      "        'test': <SplitInfo num_examples=10000, num_shards=1>,\n",
      "        'train': <SplitInfo num_examples=50000, num_shards=1>,\n",
      "    },\n",
      "    citation=\"\"\"@TECHREPORT{Krizhevsky09learningmultiple,\n",
      "        author = {Alex Krizhevsky},\n",
      "        title = {Learning multiple layers of features from tiny images},\n",
      "        institution = {},\n",
      "        year = {2009}\n",
      "    }\"\"\",\n",
      ")\n",
      "Element Spec:  {'image': TensorSpec(shape=(), dtype=tf.string, name=None), 'label': TensorSpec(shape=(), dtype=tf.int64, name=None)}\n",
      "Training samples:  50000\n",
      "Testing samples:  10000\n"
     ]
    }
   ],
   "source": [
    "# \"\"\"Load the dataset: Public or Local\"\"\"\n",
    "# if tf.io.gfile.isdir(DATASET):\n",
    "#     repo = DatasetRepository(data_dir=DATASET)\n",
    "#     builder = repo.get_builder()  # Builds all versions by default\n",
    "#     ds_info = builder.info\n",
    "#     (raw_train_ds, raw_test_ds) = builder.as_dataset(split=['train', 'test'],\n",
    "#                                                      decoders=Decoders.SIMPLE_DECODER)\n",
    "# else:\n",
    "# Load TFDS dataset by name (publicly-hosted on TF)\n",
    "(raw_train_ds, raw_test_ds), ds_info = tfds.load(DATASET,\n",
    "                                                 split=['train', 'test'],\n",
    "                                                 with_info=True,\n",
    "                                                 decoders=Decoders.SIMPLE_DECODER)\n",
    "print('About: ', ds_info)\n",
    "print('Element Spec: ', raw_train_ds.element_spec)\n",
    "print('Training samples: ', len(raw_train_ds))\n",
    "print('Testing samples: ', len(raw_test_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5639e791-734a-479e-8aea-5af4a6553e6b",
   "metadata": {},
   "source": [
    "Define Feature Extractor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33ad82be-5352-45ff-9c64-02a5a9e3e396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"feature_extractor\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " efficientnetv2-b0 (Function  (None, 1280)             5919312   \n",
      " al)                                                             \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,919,312\n",
      "Trainable params: 0\n",
      "Non-trainable params: 5,919,312\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "backbone = tf.keras.applications.EfficientNetV2B0(\n",
    "            include_top=False,\n",
    "            weights='imagenet',\n",
    "            input_shape=(*IMG_SIZE, 3),\n",
    "            pooling='avg'\n",
    "        )\n",
    "backbone.trainable = False\n",
    "\n",
    "\"\"\"Add augmentation/input layers\"\"\"\n",
    "feature_extractor = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(backbone.input_shape[1:]),\n",
    "    backbone,\n",
    "], name='feature_extractor')\n",
    "\n",
    "feature_extractor.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32eeeb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_ds = (test_features\n",
    "#                     .cache()\n",
    "#                     .map(as_tuple(x='image', y='label'))\n",
    "#                     .batch(BATCH_SIZE)\n",
    "#                     .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "137fd579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3bbb218",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15254fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c4fd73cf-a4c4-4d37-9052-a5130a99af7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"\"\"Extract train/test feature embeddings\"\"\"\n",
    "# print(f'Extracting train set features')\n",
    "# train_features = extract_features(dataset=(raw_train_ds\n",
    "#                                         .map(decode_example(IMG_SIZE))\n",
    "#                                         .map(as_tuple(x='image', y='label'))\n",
    "#                                         .batch(BATCH_SIZE)\n",
    "#                                         .prefetch(tf.data.AUTOTUNE)), model=feature_extractor)\n",
    "# print(f'Extracting test set features')\n",
    "# test_features = extract_features(dataset=(raw_test_ds\n",
    "#                                         .map(decode_example(IMG_SIZE))\n",
    "#                                         .map(as_tuple(x='image', y='label'))\n",
    "#                                         .batch(BATCH_SIZE)\n",
    "#                                         .prefetch(tf.data.AUTOTUNE)), model=feature_extractor)\n",
    "# print('Features Dataset spec: ', train_features.element_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3da995ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioned_dataset = synthesize_by_sharding_over_labels(train_features, \n",
    "#                                                          num_partitions=4,\n",
    "# #                                                          shuffle_labels=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "861fa5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a7774f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# valid_ds = (test_features\n",
    "#                     .cache()\n",
    "#                     .map(as_tuple(x='image', y='label'))\n",
    "#                     .prefetch(tf.data.AUTOTUNE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7c2be3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d8a921",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f405fcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(list(partitioned_dataset[]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3a434a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Clients:', len(partitioned_dataset))\n",
    "# for client_id in partitioned_dataset:\n",
    "#     dist = get_label_distribution(partitioned_dataset[client_id])\n",
    "#     print(f'Client {client_id}: {dist}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02b6ed3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# partitioned_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8c9eb50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openfl.interface.interactive_api.experiment import DataInterface\n",
    "\n",
    "class CIFAR10FedDataset(DataInterface):\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    @property\n",
    "    def shard_descriptor(self):\n",
    "        return self._shard_descriptor\n",
    "\n",
    "    @shard_descriptor.setter\n",
    "    def shard_descriptor(self, shard_descriptor):\n",
    "        \"\"\"\n",
    "        Describe per-collaborator procedures or sharding.\n",
    "\n",
    "        This method will be called during a collaborator initialization.\n",
    "        Local shard_descriptor will be set by Envoy.\n",
    "        \"\"\"\n",
    "        self._shard_descriptor = shard_descriptor\n",
    "        \n",
    "        # shard_descriptor.get_split(...) returns a tf.data.Dataset\n",
    "        # Check cifar10_shard_descriptor.py for details\n",
    "        \n",
    "        self.train_set = self._shard_descriptor.get_split('train')\n",
    "        self.valid_set = self._shard_descriptor.get_split('valid')\n",
    "        self.train_size = self._shard_descriptor.get_split('train_size')\n",
    "        self.valid_size = self._shard_descriptor.get_split('test_size')\n",
    "        \n",
    "    def get_train_loader(self):\n",
    "        \"\"\"Output of this method will be provided to tasks with optimizer in contract\"\"\"\n",
    "        return self.train_set\n",
    "\n",
    "    def get_valid_loader(self):\n",
    "        \"\"\"Output of this method will be provided to tasks without optimizer in contract\"\"\"\n",
    "        return self.valid_set\n",
    "    \n",
    "    def get_train_data_size(self) -> int:\n",
    "        \"\"\"Information for aggregation\"\"\"\n",
    "        return self.train_size\n",
    "\n",
    "    def get_valid_data_size(self) -> int:\n",
    "        \"\"\"Information for aggregation\"\"\"\n",
    "        return self.valid_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dfb459",
   "metadata": {},
   "source": [
    "### Create CIFAR10 federated dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4af5c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_dataset = CIFAR10FedDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b469d9c7-53b0-4e12-9a40-224016914d51",
   "metadata": {},
   "source": [
    "Register Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4942e600-8940-4851-8693-3385c1d48918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BEFORE BUILD SLDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 10:26:36.004181: I tensorflow/core/util/cuda_solvers.cc:179] Creating GpuSolver handles for stream 0x81ec2a0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AFTER BUILD SLDA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-20 10:26:38.383679: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    }
   ],
   "source": [
    "# # Define model\n",
    "# model = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(10, activation=None),\n",
    "# ], name='simplecnn')\n",
    "# model.summary()\n",
    "\n",
    "# # Define optimizer\n",
    "# optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n",
    "\n",
    "# # Loss and metrics. These will be used later.\n",
    "# loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# train_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "# val_acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "\n",
    "model = SLDA(n_components=feature_extractor.output_shape[-1],\n",
    "             num_classes=ds_info.features['label'].num_classes)\n",
    "\n",
    "model.compile(metrics=['accuracy'])\n",
    "# Create ModelInterface\n",
    "framework_adapter = 'openfl.plugins.frameworks_adapters.keras_adapter.FrameworkAdapterPlugin'\n",
    "MI = ModelInterface(model=model, optimizer=None, framework_plugin=framework_adapter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ff53a697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "66509498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_model = copy.deepcopy(model)\n",
    "# res_model.__class__ = SLDA\n",
    "# res_model.compile(metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed06f18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ac6965e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'means:0' shape=(10, 1280) dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'counts:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'sigma:0' shape=(1280, 1280) dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'sigma_inv:0' shape=(1280, 1280) dtype=float32, numpy=\n",
       " array([[10000.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0., 10000.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0.,     0., 10000., ...,     0.,     0.,     0.],\n",
       "        ...,\n",
       "        [    0.,     0.,     0., ..., 10000.,     0.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0., 10000.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0.,     0., 10000.]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'steps:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9879fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "34b53216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.array(res_model.get_weights())         # save weights in a np.array of np.arrays\n",
    "# res_model.set_weights(a + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cff9bd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d5870f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.set_weights(res_model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ccb22bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e779803b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c24000",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1a1eae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5ea9261d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'somefilemodelinterface'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b5a747e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(filepath, 'wb') as f:\n",
    "#     cloudpickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6c8ce503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cloudpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f05908f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'model_ckpt_path'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5835de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # with open(filepath, 'wb') as f:\n",
    "#     cloudpickle.dump(model, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5877da6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model1 = tf.keras.Sequential([\n",
    "#     tf.keras.layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "#     tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "#     tf.keras.layers.BatchNormalization(),\n",
    "#     tf.keras.layers.Flatten(),\n",
    "#     tf.keras.layers.Dense(10, activation=None),\n",
    "# ], name='simplecnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78260966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(filepath, 'wb') as f:\n",
    "#     cloudpickle.dump(model1, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f84184d",
   "metadata": {},
   "source": [
    "Test Serializer and Deserializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d364a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from importlib import import_module\n",
    "# from os.path import splitext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "60fe9b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plan_build(template, settings):\n",
    "#         \"\"\"\n",
    "#         Create an instance of a openfl Component or Federated DataLoader/TaskRunner.\n",
    "\n",
    "#         Args:\n",
    "#             template: Fully qualified class template path\n",
    "#             settings: Keyword arguments to class constructor\n",
    "\n",
    "#         Returns:\n",
    "#             A Python object\n",
    "#         \"\"\"\n",
    "#         class_name = splitext(template)[1].strip('.')\n",
    "#         module_path = splitext(template)[0]\n",
    "\n",
    "# #         Plan.logger.info(f'Building [red]🡆[/] Object [red]{class_name}[/] '\n",
    "# #                          f'from [red]{module_path}[/] Module.',\n",
    "# #                          extra={'markup': True})\n",
    "# #         Plan.logger.debug(f'Settings [red]🡆[/] {settings}',\n",
    "# #                           extra={'markup': True})\n",
    "# #         Plan.logger.debug(f'Override [red]🡆[/] {override}',\n",
    "# #                           extra={'markup': True})\n",
    "\n",
    "# #         settings.update(**override)\n",
    "\n",
    "#         module = import_module(module_path)\n",
    "#         instance = getattr(module, class_name)(**settings)\n",
    "\n",
    "#         return instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fd880923",
   "metadata": {},
   "outputs": [],
   "source": [
    "# serializer = plan_build('openfl.plugins.interface_serializer.cloudpickle_serializer.CloudpickleSerializer', {})\n",
    "# framework_adapter = plan_build('openfl.plugins.frameworks_adapters.keras_adapter.FrameworkAdapterPlugin', {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "17170823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framework_adapter.serialization_setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "543213d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# framework_adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1e780c05",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# serializer.serialize(model, 'custom_loader_ob1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2dfde5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# m = serializer.restore_object('custom_loader_ob1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "437a4a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "381bb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# type(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "02191278",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow import keras\n",
    "# import tensorflow as tf\n",
    "# import cloudpickle\n",
    "\n",
    "# filename = 'custom_loader_ob1.pkl'\n",
    "\n",
    "# with open(filename, 'rb') as f:\n",
    "#     mi = cloudpickle.load(f)\n",
    "\n",
    "# print(type(mi))\n",
    "# print(mi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "30d12149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mi.model.compile(metrics=['accuracy'])\n",
    "\n",
    "# print(mi.model.get_weights())\n",
    "\n",
    "# mi.model.fit(tf.random.uniform((1, 1280)), tf.random.uniform((1,), minval=0, maxval=10, dtype=tf.int64))\n",
    "\n",
    "# val = mi.model.evaluate(tf.random.uniform((32, 1280)), tf.random.uniform((32,), minval=0, maxval=10, dtype=tf.int64))\n",
    "# # val = mi.model.evaluate(tf.random.uniform((32, 1280)))\n",
    "\n",
    "# print(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57381ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "849c165b",
   "metadata": {},
   "source": [
    "## Define and register FL tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b9649385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Progbar\n",
    "from openfl.interface.aggregation_functions import FedSLDAAggregation\n",
    "\n",
    "agg_fn = FedSLDAAggregation()\n",
    "TI = TaskInterface()\n",
    "\n",
    "@TI.register_fl_task(model='model', data_loader='dataset', optimizer='optimizer', device='device')\n",
    "@TI.set_aggregation_function(agg_fn)\n",
    "def train(model, dataset, optimizer, device, warmup=False):\n",
    "    res_model = SLDA(1280, 10)\n",
    "    res_model.set_weights(model.get_weights())\n",
    "    res_model.compile(metrics=['accuracy'])\n",
    "    res_model.fit(dataset, epochs=1)\n",
    "    train_acc = res_model.evaluate(dataset.unbatch().batch(128))\n",
    "    \n",
    "    # Exit\n",
    "    model.set_weights(res_model.get_weights())\n",
    "    return {'train_acc': train_acc,}\n",
    "\n",
    "\n",
    "@TI.register_fl_task(model='model', data_loader='dataset', device='device')     \n",
    "def validate(model, dataset, device):\n",
    "    # Run a validation loop at the end of each epoch.\n",
    "    res_model = SLDA(1280, 10)\n",
    "    res_model.set_weights(model.get_weights())\n",
    "    res_model.compile(metrics=['accuracy'])\n",
    "    val_acc = res_model.evaluate(dataset)\n",
    "    return {'validation_accuracy': val_acc,}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0ebf2d",
   "metadata": {},
   "source": [
    "## Time to start a federated learning experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d41b7896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an experimnet in federation\n",
    "experiment_name = 'cifar10_experiment'\n",
    "fl_experiment = FLExperiment(federation=federation, experiment_name=experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "41b44de9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://23d7aa69-3dfa-4b91-8861-267400bd042f/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://23d7aa69-3dfa-4b91-8861-267400bd042f/assets\n",
      "WARNING:openfl.federated.task.task_runner:tried to remove tensor: __opt_state_needed not present in the tensor dict\n",
      "WARNING:openfl.interface.interactive_api.experiment:tried to remove tensor: __opt_state_needed not present in the tensor dict\n"
     ]
    }
   ],
   "source": [
    "# The following command zips the workspace and python requirements to be transfered to collaborator nodes\n",
    "ROUNDS_TO_TRAIN = 10\n",
    "fl_experiment.start(model_provider=MI,\n",
    "                   task_keeper=TI,\n",
    "                   data_loader=fed_dataset,\n",
    "                   rounds_to_train=ROUNDS_TO_TRAIN,\n",
    "                   opt_treatment='CONTINUE_GLOBAL', )\n",
    "fl_experiment.stream_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "952dac3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'means:0' shape=(10, 1280) dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'counts:0' shape=(10,) dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>,\n",
       " <tf.Variable 'sigma:0' shape=(1280, 1280) dtype=float32, numpy=\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)>,\n",
       " <tf.Variable 'sigma_inv:0' shape=(1280, 1280) dtype=float32, numpy=\n",
       " array([[10000.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0., 10000.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0.,     0., 10000., ...,     0.,     0.,     0.],\n",
       "        ...,\n",
       "        [    0.,     0.,     0., ..., 10000.,     0.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0., 10000.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0.,     0., 10000.]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'steps:0' shape=() dtype=float32, numpy=0.0>,\n",
       " <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.0>]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "55b5505f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.]], dtype=float32),\n",
       " array([[10000.,     0.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0., 10000.,     0., ...,     0.,     0.,     0.],\n",
       "        [    0.,     0., 10000., ...,     0.,     0.,     0.],\n",
       "        ...,\n",
       "        [    0.,     0.,     0., ..., 10000.,     0.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0., 10000.,     0.],\n",
       "        [    0.,     0.,     0., ...,     0.,     0., 10000.]],\n",
       "       dtype=float32),\n",
       " 0.0,\n",
       " 0.0]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc2559c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "50f96e42274e6c5243b63a33fbf7578ef10c4dac29a09d4747c12b7618b0d3bc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
